# AlphaFold2çš„colabæ‰¹é‡é¢„æµ‹ä¸åµŒå…¥(Embedding)æå–(Pair_Representation, Structure)
import os                      # æ“ä½œç³»ç»Ÿæ¥å£ï¼ˆç”¨äºæ–‡ä»¶/ç›®å½•æ“ä½œï¼‰
import re                      # æ­£åˆ™è¡¨è¾¾å¼ï¼ˆç”¨äºå­—ç¬¦ä¸²æ¸…æ´—ï¼‰
import hashlib                 # å“ˆå¸Œåº“ï¼ˆç”Ÿæˆå”¯ä¸€æ ‡è¯†ï¼‰
import numpy as np             # æ•°å€¼è®¡ç®—åº“ï¼ˆå¤„ç†å¤šç»´æ•°ç»„æ•°æ®ï¼‰
from pathlib import Path       # è·¯å¾„æ“ä½œåº“ï¼ˆæä¾›é¢å‘å¯¹è±¡çš„è·¯å¾„å¤„ç†æ–¹å¼ï¼‰# Path: è·¯å¾„æ“ä½œå¯¹è±¡ï¼ˆæ¯”å­—ç¬¦ä¸²è·¯å¾„æ›´å®‰å…¨çš„æ“ä½œæ–¹å¼ï¼‰

# ColabFoldä¸»ç¨‹åºå®‰è£…
if not os.path.isfile("COLABFOLD_READY"):  # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…ColabFoldï¼ˆé€šè¿‡æ ‡å¿—æ–‡ä»¶åˆ¤æ–­ï¼‰
    print("ğŸš€ æ­£åœ¨å®‰è£…ColabFold...")
    os.system("pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'")  # å¿½ç•¥ä¾èµ–å†²çªé™é»˜å®‰è£…ColabFoldï¼ˆä¸åŒ…å«JAXï¼‰
    if os.environ.get('TPU_NAME', False):           # å¦‚æœæ£€æµ‹åˆ°TPUç¯å¢ƒ
        os.system("pip uninstall -y jax jaxlib")    # å¸è½½é»˜è®¤çš„JAXå’Œjaxlib
        os.system("pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html")  # å®‰è£…å…¼å®¹TPUçš„JAXå’ŒHaiku
    os.system("ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold")  # åˆ›å»ºcolabfoldè½¯é“¾æ¥
    os.system("ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold")  # åˆ›å»ºalphafoldè½¯é“¾æ¥
    os.system("touch COLABFOLD_READY")  # åˆ›å»ºæ ‡å¿—æ–‡ä»¶ï¼Œé¿å…é‡å¤å®‰è£…
from colabfold.download import download_alphafold_params        # æ¨¡å‹å‚æ•°ä¸‹è½½å™¨
from colabfold.batch import get_queries, run, set_model_type    # æ ¸å¿ƒé¢„æµ‹æµç¨‹ç»„ä»¶
from colabfold.utils import setup_logging                       # æ—¥å¿—é…ç½®å·¥å…·

def add_hash(x, y):
    """ç”Ÿæˆå”¯ä¸€å“ˆå¸Œæ ‡è¯†ï¼ˆå‰5ä½SHA1ï¼‰ï¼Œç”¨äºåˆ›å»ºå”¯ä¸€ä»»åŠ¡ç›®å½•å
    PythonçŸ¥è¯†ç‚¹ï¼š
        - hashlib.sha1(): åˆ›å»ºSHA1å“ˆå¸Œå¯¹è±¡
        - .encode(): å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—èŠ‚ï¼ˆå“ˆå¸Œå‡½æ•°éœ€è¦å­—èŠ‚è¾“å…¥ï¼‰
        - .hexdigest()[:5]: è·å–å“ˆå¸Œå€¼çš„å‰5ä¸ªå­—ç¬¦
    """
    return x + "_" + hashlib.sha1(y.encode()).hexdigest()[:5]   # è¿”å›æ ¼å¼ï¼š<æ¸…æ´—åçš„header>_<åºåˆ—å“ˆå¸Œå‰5ä½>

def parse_fasta(filename):
    """è§£æFASTAæ–‡ä»¶å¹¶è¿”å›{header: sequence}å­—å…¸
    å‚æ•°ï¼š
        filename: è¾“å…¥FASTAæ–‡ä»¶è·¯å¾„
    è¿”å›ï¼š
        å­—å…¸ç»“æ„ï¼š{åºåˆ—å¤´æ ‡è¯†: è›‹ç™½è´¨åºåˆ—}
    å¤„ç†é€»è¾‘ï¼š
        1. ä»¥'>'å¼€å¤´çš„è¡Œä½œä¸ºæ–°åºåˆ—å¤´
        2. è¿ç»­çš„éå¤´è¡Œåˆå¹¶ä¸ºè›‹ç™½è´¨åºåˆ—
        3. è‡ªåŠ¨å»é™¤å¤´éƒ¨çš„ç‰¹æ®Šå­—ç¬¦å’Œç©ºæ ¼
    """
    """è§£æFASTAæ–‡ä»¶...ï¼ˆä¿æŒåŸæœ‰æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰
    Pythonæ–‡ä»¶æ“ä½œï¼š
        - with open(): ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å¤„ç†æ–‡ä»¶æ‰“å¼€/å…³é—­
        - line.strip(): ç§»é™¤è¡Œé¦–å°¾ç©ºç™½å­—ç¬¦
        - .startswith('>'): æ£€æµ‹åºåˆ—å¤´æ ‡è¯†
        - .split()[0]: å–ç¬¬ä¸€ä¸ªç©ºæ ¼å‰çš„å…ƒç´ 
    """
    sequences = {}  # åˆå§‹åŒ–ç©ºå­—å…¸ç”¨äºå­˜å‚¨header:sequenceå¯¹
    with open(filename, 'r') as f:  # æ‰“å¼€FASTAæ–‡ä»¶ï¼Œè‡ªåŠ¨å…³é—­
        current_header = None       # å½“å‰åºåˆ—å¤´åˆå§‹åŒ–ä¸ºNone
        current_sequence = []       # å½“å‰åºåˆ—å†…å®¹åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
        for line in f:              # é€è¡Œè¯»å–æ–‡ä»¶
            line = line.strip()     # å»é™¤è¡Œé¦–å°¾ç©ºç™½å­—ç¬¦
            if line.startswith('>'):  # æ£€æŸ¥æ˜¯å¦ä¸ºåºåˆ—å¤´
                if current_header is not None:  # å¦‚æœå·²æœ‰headerï¼Œä¿å­˜ä¸Šä¸€æ¡åºåˆ—
                    sequences[current_header] = ''.join(current_sequence)  # åˆå¹¶åºåˆ—å¹¶å­˜å…¥å­—å…¸
                current_header = line[1:].split()[0]  # æå–headerï¼ˆå»æ‰'>'ï¼Œä»…å–ç¬¬ä¸€ä¸ªå•è¯ï¼‰
                current_sequence = []  # é‡ç½®åºåˆ—å†…å®¹
            else:
                current_sequence.append(line)  # éheaderè¡Œè¿½åŠ åˆ°åºåˆ—å†…å®¹
        if current_header is not None:  # æ–‡ä»¶ç»“æŸåä¿å­˜æœ€åä¸€æ¡åºåˆ—
            sequences[current_header] = ''.join(current_sequence)
    return sequences  # è¿”å›header:sequenceå­—å…¸

# é…ç½®å‚æ•°ï¼ˆæ ¸å¿ƒå‚æ•°è¯´æ˜ï¼‰
fasta_file = "/content/Others_TED.fasta"    # è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œéœ€åŒ…å«æ ‡å‡†FASTAæ ¼å¼åºåˆ— # å­—ç¬¦ä¸²ç±»å‹
base_model_type = "auto"                    # æ¨¡å‹è‡ªåŠ¨é€‰æ‹©ç­–ç•¥ï¼ˆæ ¹æ®åºåˆ—å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©å•ä½“/å¤åˆä½“æ¨¡å‹ï¼‰ # å­—ç¬¦ä¸²æšä¸¾å€¼ï¼ˆå¯é€‰"auto", "monomer", "multimer"ï¼‰
num_models = 1                              # æ¯ä¸ªåºåˆ—è¿è¡Œçš„æ¨¡å‹æ•°é‡ï¼ˆå¢åŠ å¯æå‡å‡†ç¡®æ€§ä½†å»¶é•¿è®¡ç®—æ—¶é—´ï¼‰ # æ•´æ•°ç±»å‹ï¼Œå¿…é¡»å¤§äº0
num_recycles = 3                            # å¾ªç¯è¿­ä»£æ¬¡æ•°ï¼ˆ3-6ä¸ºæ¨èå€¼ï¼Œè¿­ä»£æ¬¡æ•°è¶Šå¤šç²¾åº¦å¯èƒ½è¶Šé«˜ï¼‰ # æ•´æ•°ç±»å‹ï¼Œå…¸å‹èŒƒå›´3-6
msa_mode = "mmseqs2_uniref_env"             # MSAç”Ÿæˆæ–¹å¼ï¼ˆå¹³è¡¡é€Ÿåº¦ä¸ç²¾åº¦çš„æ¨èæ¨¡å¼ï¼‰ # å­—ç¬¦ä¸²æšä¸¾å€¼ï¼ˆæ§åˆ¶MSAç”Ÿæˆç®—æ³•ï¼‰
extract_embeddings = True                   # å¯ç”¨åµŒå…¥æå–åŠŸèƒ½ï¼ˆè®¾ä¸ºFalseå¯åŠ å¿«é¢„æµ‹é€Ÿåº¦ï¼‰ # å¸ƒå°”ç±»å‹ï¼ˆTrue/Falseï¼‰
embedding_types = ["msa", "pair", "structure"]  # åˆ—è¡¨ç±»å‹ï¼ŒåŒ…å«é¢„å®šä¹‰å­—ç¬¦ä¸²ï¼ˆ"msa", "pair", "structure"ï¼‰
                                                # éœ€è¦æå–çš„åµŒå…¥ç±»å‹è¯´æ˜ï¼š
                                                # msa: å¤šåºåˆ—æ¯”å¯¹åµŒå…¥
                                                # pair: æ®‹åŸºå¯¹è¡¨ç¤º
                                                # structure: ç»“æ„æ¨¡å—åµŒå…¥

# è§£æFastaæ–‡ä»¶
sequences = parse_fasta(fasta_file)     # è¿”å›å­—å…¸å¯¹è±¡ï¼Œé”®å€¼å¯¹ä¸ºheader:sequence# å­—å…¸ç»“æ„ï¼š{åºåˆ—å¤´æ ‡è¯†: è›‹ç™½è´¨åºåˆ—}
if not sequences:                       # ç©ºå­—å…¸åˆ¤æ–­ ï¼ˆæ— åºåˆ—æ—¶ï¼‰
    raise ValueError(f"No sequences found in {fasta_file}")  # æŠ›å‡ºå¼‚å¸¸å¹¶ç»ˆæ­¢ç¨‹åº

# ä¸»å¾ªç¯æµç¨‹ï¼ˆæ–°å¢å¾ªç¯ç»“æ„è§£é‡Šï¼‰
'''
sequences.items(): éå†å­—å…¸çš„é”®å€¼å¯¹ï¼ˆheader, sequenceï¼‰
enumerateå¯åŠ ï¼šfor i, (header, sequence) in enumerate(sequences.items()):
''' 
# å¤„ç†æ¯ä¸ªåºåˆ—çš„ä¸»æµç¨‹
for header, sequence in sequences.items():
    print(f"\nğŸš€ æ­£åœ¨å¤„ç†åºåˆ—: {header}")  # f-stringæ ¼å¼åŒ–è¾“å‡º
    
    '''
    ç›®å½•å‘½åè§„åˆ™ï¼š
    re.sub(r'\W+', '', header): ç§»é™¤éå­—æ¯æ•°å­—å­—ç¬¦ï¼ˆ\Wè¡¨ç¤ºéå•è¯å­—ç¬¦ï¼‰
    add_hash(): æ·»åŠ åºåˆ—å“ˆå¸Œé˜²æ­¢åç§°å†²çª(!!!!!!æœ¬æ®µæˆ–è®¸å¯ä»¥åˆ é™¤ï¼ï¼ï¼ï¼ï¼)
    '''
    jobname = add_hash(re.sub(r'\W+', '', header), sequence)    # åˆ›å»ºä»»åŠ¡ç›®å½•ï¼ˆç›®å½•åæ ¼å¼ï¼šæ¸…æ´—åçš„header_åºåˆ—å“ˆå¸Œï¼‰
    os.makedirs(jobname, exist_ok=True)    # os.makedirsï¼šé€’å½’åˆ›å»ºç›®å½•ï¼Œexist_ok=Trueï¼šç›®å½•å·²å­˜åœ¨æ—¶ä¸æŠ¥é”™
    print(f"âœ… ä»»åŠ¡ç›®å½•å·²åˆ›å»º: {jobname}")  # è¾“å‡ºåˆ›å»ºçš„ä»»åŠ¡ç›®å½•å

    queries_path = os.path.join(jobname, f"{jobname}.csv")    # ä¿å­˜æŸ¥è¯¢åºåˆ—ï¼ˆCSVæ ¼å¼ï¼Œç”¨äºåç»­å¤„ç†ï¼ï¼ï¼ï¼ï¼ï¼æˆ–è®¸ä¹Ÿå¯ä»¥åˆ é™¤ï¼ï¼ï¼ï¼ï¼ï¼‰# CSVæ ¼å¼ï¼šæ¯è¡Œè¡¨ç¤ºä¸€æ¡è®°å½•ï¼Œé€—å·åˆ†éš”å€¼
    with open(queries_path, "w") as f: # 'w'æ¨¡å¼è¡¨ç¤ºå†™å…¥ï¼ˆä¼šè¦†ç›–å·²æœ‰æ–‡ä»¶ï¼‰
        '''
        å†™å…¥CSVæ–‡ä»¶ï¼ˆæ–°å¢è¯´æ˜ï¼‰ï¼š
        f.write(f"id,sequence\n{jobname},{sequence}")ï¼šå†™å…¥CSVæ–‡ä»¶ï¼ˆåŒ…å«idå’Œsequenceä¸¤åˆ—ï¼‰
        '''
        f.write(f"id,sequence\n{jobname},{sequence}")   # å†™å…¥CSVæ–‡ä»¶ï¼ˆåŒ…å«idå’Œsequenceä¸¤åˆ—ï¼‰
    log_filename = os.path.join(jobname, "log.txt")     # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    setup_logging(Path(log_filename))                   # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼ˆè®°å½•é¢„æµ‹è¿‡ç¨‹ä¸­çš„è¯¦ç»†ä¿¡æ¯ï¼‰
    print(f"ğŸ” å·²ä¿å­˜æŸ¥è¯¢åºåˆ—åˆ°: {queries_path}")    
    queries, is_complex = get_queries(queries_path)     # è§£æè¾“å…¥åºåˆ—
    specific_model_type = set_model_type(is_complex, base_model_type)     # æ¨¡å‹ç±»å‹åˆ¤æ–­é€»è¾‘ï¼ˆè‡ªåŠ¨æ£€æµ‹å•ä½“/å¤åˆä½“ç»“æ„ï¼‰ # ç¡®å®šæœ€ç»ˆæ¨¡å‹ç±»å‹
    print(f"ğŸ” è§£æç»“æœ: {len(queries)} æ¡æŸ¥è¯¢åºåˆ—, æ˜¯å¦ä¸ºå¤åˆä½“: {is_complex}, æ¨¡å‹ç±»å‹: {specific_model_type}")
    '''
    æ¨¡å‹ç±»å‹åˆ¤æ–­ï¼ˆæ–°å¢ç±»å‹è¯´æ˜ï¼‰ï¼š
    is_complexï¼šå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ä¸ºå¤åˆä½“ï¼ˆå¤šä¸ªé“¾çš„ç›¸äº’ä½œç”¨ï¼‰
    set_model_typeï¼šè¿”å›å…·ä½“æ¨¡å‹ç±»å‹å­—ç¬¦ä¸²ï¼ˆå¦‚"model_1_multimer"ï¼‰
    base_model_typeï¼šåŸºç¡€æ¨¡å‹ç±»å‹ï¼ˆ"auto"è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©ï¼‰
    '''
    print(f"ğŸš€ æ­£åœ¨ä¸‹è½½æ¨¡å‹å‚æ•°ï¼Œæ¨¡å‹ç±»å‹: {specific_model_type}")
    download_alphafold_params(specific_model_type, Path("."))    # ä¸‹è½½æ¨¡å‹å‚æ•°ï¼ˆä»Google Cloud Storageè·å–é¢„è®­ç»ƒæƒé‡ï¼‰    # ä¸‹è½½çº¦æ•°ç™¾MBçš„æ¨¡å‹å‚æ•°æ–‡ä»¶åˆ°å½“å‰ç›®å½•    # éœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥ï¼Œé¦–æ¬¡è¿è¡Œè€—æ—¶è¾ƒé•¿
    print("âœ… æ¨¡å‹å‚æ•°ä¸‹è½½å®Œæˆ!")  # ä¸‹è½½å®Œæˆæç¤º
    
    def prediction_callback(protein_obj, length, prediction_result, input_features, mode):
        """
        åµŒå…¥æå–å›è°ƒå‡½æ•°
        å‚æ•°ï¼š  prediction_result: åŒ…å«æ¨¡å‹è¾“å‡ºçš„å­—å…¸
                mode: å½“å‰æ¨¡å‹ä¿¡æ¯ï¼ˆæ¨¡å‹åç§°ï¼Œæ¨¡å‹ç¼–å·ï¼‰
        åŠŸèƒ½ï¼š  ä»é¢„æµ‹ç»“æœä¸­æå–æŒ‡å®šç±»å‹çš„åµŒå…¥è¡¨ç¤ºï¼Œä¿å­˜ä¸º.npyæ–‡ä»¶
        æ³¨æ„ï¼š  - è¯¥å‡½æ•°åœ¨æ¯ä¸ªæ¨¡å‹é¢„æµ‹å®Œæˆåè‡ªåŠ¨è°ƒç”¨
                - prediction_resultåŒ…å«æ¨¡å‹ä¸­é—´è¡¨ç¤º
        AlphaFold2æœºåˆ¶è¯´æ˜ï¼šè¯¥å‡½æ•°åœ¨æ¯ä¸ªæ¨¡å‹é¢„æµ‹å®Œæˆåè‡ªåŠ¨è°ƒç”¨ï¼Œprediction_resultåŒ…å«æ¨¡å‹ä¸­é—´è¡¨ç¤º

        """
        if not extract_embeddings:
            return

        # å­—å…¸æ“ä½œè¯´æ˜ï¼š
        model_name, _ = mode
        embeddings = {} # åˆ›å»ºç©ºå­—å…¸å­˜å‚¨åµŒå…¥æ•°æ®

        # æ£€æŸ¥åµŒå…¥ç±»å‹æ˜¯å¦åœ¨æŒ‡å®šåˆ—è¡¨ä¸­
        print("-----------------------")
        print("å·²è°ƒç”¨ prediction_callbackï¼Œç»“æœåŒ…å«çš„é”®:", prediction_result.keys())  # æ‰“å°é¢„æµ‹ç»“æœçš„é”®å€¼ï¼ˆè°ƒè¯•ç”¨ï¼‰
        print("prediction_result æ˜¯å¦åŒ…å« representations:", "representations" in prediction_result)
        if "msa" in embedding_types and "representations" in prediction_result:
            if "msa" in prediction_result["representations"]:
                embeddings["msa"] = prediction_result["representations"]["msa"]
            else:
                print("è­¦å‘Š: æœªåœ¨ prediction_result['representations'] ä¸­æ‰¾åˆ° 'msa' åµŒå…¥")
        if "pair" in embedding_types and "representations" in prediction_result:
            if "pair" in prediction_result["representations"]:
                embeddings["pair"] = prediction_result["representations"]["pair"]
                print("*****************************************************")
                print(f"Pair åµŒå…¥å¼ é‡å½¢çŠ¶: {embeddings['pair'].shape}")
            else:
                print("è­¦å‘Š: æœªåœ¨ prediction_result['representations'] ä¸­æ‰¾åˆ° 'pair' åµŒå…¥")
        if "structure" in embedding_types and "structure_module" in prediction_result:
            embeddings["structure"] = prediction_result["structure_module"]
            print("*****************************************************")
        # ä¿å­˜åµŒå…¥æ–‡ä»¶ï¼ˆæ¯ä¸ªæ¨¡å‹å•ç‹¬ä¿å­˜ï¼‰
        if embeddings:
            for embed_type, embed_data in embeddings.items():
                np.save(
                    os.path.join(jobname, f"{jobname}_{embed_type}_embeddings_{model_name}.npy"),
                    embed_data  # åµŒå…¥æ•°æ®ç»´åº¦è¯´æ˜ï¼š
                                # msa: [åºåˆ—é•¿åº¦, MSAæ·±åº¦, åµŒå…¥ç»´åº¦]
                                # pair: [åºåˆ—é•¿åº¦, åºåˆ—é•¿åº¦, é€šé“æ•°]
                                # structure: [åºåˆ—é•¿åº¦, åµŒå…¥ç»´åº¦]
                )
            print(f"å·²ä¿å­˜ {header} çš„ {model_name} åµŒå…¥æ–‡ä»¶")
        # numpyæ–‡ä»¶æ“ä½œï¼š
        # np.save()ï¼šå°†numpyæ•°ç»„ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶
        # åŠ è½½æ—¶ä½¿ç”¨ï¼šnp.load("filename.npy")
        
    # æ ¸å¿ƒé¢„æµ‹æµç¨‹ï¼ˆä½¿ç”¨AlphaFold2è¿›è¡Œç»“æ„é¢„æµ‹ï¼‰
    print(f"ğŸš€ æ­£åœ¨è¿è¡Œ {header} çš„ç»“æ„é¢„æµ‹...")
    results = run(
        queries=queries, # ä»CSVè§£æçš„æŸ¥è¯¢åˆ—è¡¨
        result_dir=jobname,
        use_templates=False,                        # ç¦ç”¨æ¨¡æ¿ï¼ˆæå‡é€Ÿåº¦ä½†å¯èƒ½å½±å“å‡†ç¡®æ€§ï¼‰
        num_relax=0,                                # ç»“æ„æ¾å¼›æ¬¡æ•°ï¼ˆ0è¡¨ç¤ºä¸è¿›è¡Œï¼Œå¯è®¾ä¸º1ï¼‰
        msa_mode=msa_mode,                          # æ§åˆ¶MSAç”Ÿæˆæ–¹å¼ï¼ˆå½±å“é€Ÿåº¦ä¸ç²¾åº¦å¹³è¡¡ï¼‰
        model_type=specific_model_type,             # ä½¿ç”¨è‡ªåŠ¨é€‰æ‹©çš„æ¨¡å‹ç±»å‹
        num_models=num_models,                      # æ¯ä¸ªåºåˆ—è¿è¡Œçš„æ¨¡å‹æ•°é‡ï¼ˆ1-5ï¼‰
        num_recycles=num_recycles,                  # å¾ªç¯æ¬¡æ•°å½±å“æœ€ç»ˆç²¾åº¦
        num_seeds=1,                                # éšæœºç§å­æ•°ï¼ˆå¢åŠ å¯æå‡å¤šæ ·æ€§ï¼‰
        model_order=[1],                            # ä½¿ç”¨çš„æ¨¡å‹ç¼–å·ï¼ˆAlphaFoldæä¾›çš„ä¸åŒè®­ç»ƒè½®æ¬¡çš„æ¨¡å‹ï¼‰
        is_complex=is_complex,                      # æ˜¯å¦ä¸ºå¤åˆä½“é¢„æµ‹ï¼ˆå½±å“æ¨¡å‹é€‰æ‹©ï¼‰
        data_dir=Path("."),                         # æ•°æ®ç›®å½•ï¼ˆæ¨¡å‹å‚æ•°ä¸‹è½½ç›®å½•ï¼‰
        keep_existing_results=False,                # æ˜¯å¦ä¿ç•™å·²æœ‰ç»“æœï¼ˆTrueè¡¨ç¤ºè·³è¿‡å·²å­˜åœ¨çš„é¢„æµ‹ï¼‰
        rank_by="auto",                             # ç»“æœæ’åºæ–¹å¼ï¼ˆè‡ªåŠ¨é€‰æ‹©pLDDTæˆ–ipTMï¼‰
        pair_mode="unpaired_paired",                # é…å¯¹æ¨¡å¼ï¼ˆé€‚ç”¨äºå•ä½“/å¤åˆä½“é¢„æµ‹ï¼‰
        prediction_callback=prediction_callback,    # åµŒå…¥æå–å›è°ƒå‡½æ•°
        zip_results=False,                          # æ˜¯å¦å‹ç¼©ç»“æœï¼ˆèŠ‚çœå­˜å‚¨ç©ºé—´ï¼‰
        save_all=True,                              # æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœï¼ˆTrueä¼šæ˜¾è‘—å¢åŠ å­˜å‚¨éœ€æ±‚ï¼‰
        use_cluster_profile=True,                   # ä½¿ç”¨èšç±»é…ç½®æ–‡ä»¶ï¼ˆæå‡MSAè´¨é‡ï¼‰
        return_representations=True,                # è¿”å›åµŒå…¥è¡¨ç¤º
        return_predictions=True,                    # è¿”å›å®Œæ•´é¢„æµ‹ç»“æœ
    )

print("\nğŸ‰ æ‰€æœ‰åºåˆ—å¤„ç†å®Œæˆ!")